
<script src="https://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<!-- css file modified from https://taesung.me/SwappingAutoencoder/ -->
<style type="text/css">
  body {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1000px;
  }
  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
    0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
    5px 5px 0 0px #fff, /* The second layer */
    5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
    10px 10px 0 0px #fff, /* The third layer */
    10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
    15px 15px 0 0px #fff, /* The fourth layer */
    15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
    20px 20px 0 0px #fff, /* The fifth layer */
    20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
    25px 25px 0 0px #fff, /* The fifth layer */
    25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }

  .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
    0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

    margin-left: 10px;
    margin-right: 45px;
  }

  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
    0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
    5px 5px 0 0px #fff, /* The second layer */
    5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
    10px 10px 0 0px #fff, /* The third layer */
    10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
    top: 50%;
    transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

<!-- ----------------------------------------------------------------------------- -->

<html>

  <!-- ----------------------------------------------------------------------------- -->

  <head>
    <title>Neural 3D Video Synthesis</title>
    <meta property="og:image" content="" />
    <meta property="og:title" content="Neural 3D Video Synthesis from Multi-view Video" />
  </head>

  <!-- ----------------------------------------------------------------------------- -->

  <body>
    <br>
    <!-- <center> -->
      <center><span style="font-size:45px">Neural 3D Video Synthesis from Multi-view Video</span></center>
      <br>
      <center><span style="font-size:25px">CVPR 2022 (oral)</span></center>
      <br>
      <table align=center width=900px>
        <tr>
          <td align=center width=250px>
            <span style="font-size:20px"><a href="https://tianyeli.github.io/">Tianye Li</a><sup>1,3,*</sup></span>&nbsp;&nbsp;&nbsp;
            <span style="font-size:20px"><a href="https://scholar.google.de/citations?user=cJKcPcIAAAAJ&hl=en">Mira Slavcheva</a><sup>1,*</sup></span>&nbsp;&nbsp;&nbsp;
            <span style="font-size:20px"><a href="https://zollhoefer.com/">Michael Zollhoefer</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;<br>
            <span style="font-size:20px"><a href="https://scholar.google.com/citations?user=VmHZtsEAAAAJ&hl=en">Simon Green</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
            <span style="font-size:20px"><a href="https://christophlassner.de/">Christoph Lassner</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
            <span style="font-size:20px"><a href="https://changilkim.com/">Changil Kim</a><sup>2</sup></span>&nbsp;&nbsp;&nbsp;
            <span style="font-size:20px"><a href="https://tschmidt23.github.io/">Tanner Schmidt</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;<br>
            <span style="font-size:20px"><a href="https://scholar.google.com/citations?hl=en&user=JVum8voAAAAJ">Steven Lovegrove</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
            <span style="font-size:20px"><a href="https://scholar.google.com/citations?user=56UhAooAAAAJ&hl=en">Michael Goesele</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
            <span style="font-size:20px"><a href="https://scholar.google.com/citations?user=MhowvPkAAAAJ">Richard Newcombe</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;            
            <span style="font-size:20px"><a href="https://lvzhaoyang.github.io/">Zhaoyang Lv</a><sup>1</sup></span>
          </td>
        </tr>
      </table>

      <table align=center width=1000px>
        <tr>
          <td align=center width=100px>
            <center>
              <span style="font-size:20px"></span>
            </center>
          </td>
          <td align=center width=350px>
            <center>
              <span style="font-size:20px"><sup>1</sup>Reality Labs Research</span>
            </center>
          </td>
          <td align=center width=50px>
            <center>
              <span style="font-size:20px"><sup>2</sup>Meta</span>
            </center>
          </td>
          <td align=center width=400px>
            <center>
              <span style="font-size:20px"><sup>3</sup>University of Southern California</span>
            </center>
          </td>
          <td align=center width=100px>
            <center>
              <span style="font-size:20px"></span>
            </center>
          </td>
        </tr>
      </table>

      <center><span style="font-size:20px;line-height:2.0">*equal contributions</span><br></center>

      <center>
      <table>
        <tr>
          <td style="vertical-align:bottom" width=150px> <span style="font-size:15pt">
            <center>
              <a href="./resources/paper.pdf">[Paper]</a>
            </center>
          </td>
          <td style="vertical-align:bottom" width=150px> <span style="font-size:15pt">
            <center>
              <a href="https://arxiv.org/abs/2103.02597">[ArXiv]</a>
            </center>
          </td>
          <td style="vertical-align:bottom" width=150px> <span style="font-size:15pt">
            <center>
              <a href="./resources/supp.pdf">[Sup. Mat.]</a>
            </center>
          </td>
          <td style="vertical-align:bottom" width=150px> <span style="font-size:15pt">
            <center>
              <a href="https://github.com/facebookresearch/Neural_3D_Video">[Data]</a>
            </center>
          </td>
        </tr>
      </table>
      </center>

      <br>
      <hr>

    <!-- ----------------------------------------------------------------------------- -->

    <table align=center width=900px>
      <center><h2>Abstract</h2></center>
      <tr>
        We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation.
        Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting.
        At the core of our approach is a novel time-conditioned neural radiance fields that represents scene dynamics using a set of compact latent codes.
        To exploit the fact that changes between adjacent frames of a video are typically small and locally consistent, we propose two novel strategies for efficient training of our neural network: 1) An efficient hierarchical training scheme, and 2) an importance sampling strategy that selects the next rays for training based on the temporal variation of the input videos.
        In combination, these two strategies significantly boost the training speed, lead to fast convergence of the training process, and enable high quality results. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of just 28MB.
        We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for highly complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the current state of the art.
        Project website: <a href="https://neural-3d-video.github.io">https://neural-3d-video.github.io</a>.
        <br>
      </tr>
      <br>

      <hr>
    <!-- ----------------------------------------------------------------------------- -->

      <center><h2>Video</h2></center>

      <table align=center width=900px>
        <tr>
          <td align=center width=900px>
            <center>
              <video width="900px" autoplay loop mute playsinline controls>
                <source src="./resources/video.mp4" type="video/mp4">
                  Your browser does not support the video tag.
              </video>
            </center>
          </td>
        </tr>
      </table>

      <br>

      <hr>
    <!-- ----------------------------------------------------------------------------- -->

      <center><h2>Acknowledgements</h2></center>

      <table align=center width=900px>
        <tr>
          We thank Rick Szeliski, Anton Kaplanyan, Brian Cabral, Zhao Dong, Samir Aroudj for providing feedback to this project, Daniel Andersen for helping with photorealism evaluation, Joey Conrad and Shobhit Verma
for designing and building our capture rig.
          We thank the website template from <a href="https://taesung.me/SwappingAutoencoder/">here</a>.
        </tr>
      </table>

      <br>

  </body>

</html>
